import vtk
from vtk.util import numpy_support
from utils import *
import numpy as np
import torch
import skimage
from skimage.transform import resize
from skimage.io import imread, imsave
from skimage import data,img_as_float
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F
import os

class ScalarDataSet():
	def __init__(self,args):
		self.dataset = args.dataset
		self.batch_size = args.batch_size
		self.application = args.application
		self.interval = args.interval
		self.scale = args.scale
		self.factor = args.factor
		if self.dataset == 'temporal':
			self.dim = [250,250,50]
			self.total_samples = 48
			self.data_path = '/data/Isabel_Pressure_all_timesteps'
		if self.dataset == 'spatial':
			self.dim = [250,250,50]
			self.total_samples = 48
			self.data_path = '/data/Isabel_Pressure_all_timesteps'

		if not os.path.exists(args.result_path+args.dataset):
			os.mkdir(args.result_path+args.dataset)

		if not os.path.exists(args.model_path+args.dataset):
			os.mkdir(args.model_path+args.dataset)

		if self.application == 'extrapolation':
			self.training_samples = self.total_samples*8//10
			self.samples = range(1,self.training_samples+1)
		elif self.application == 'temporal':
			if self.dataset != 'Earthquake':
				self.samples = [i for i in range(1,self.total_samples+1,self.interval+1)]
				print(self.samples)
				self.total_samples = self.samples[-1]
				print(self.total_samples)
			else:
				self.samples = np.fromfile('/afs/crc.nd.edu/user/j/jhan5/CoordNet/earthquake_select.dat',dtype='int16')
				self.samples[0] = 1
		elif self.application == 'spatial':
			self.samples = range(1,self.total_samples+1)
		elif self.application == 'super-spatial':
			if self.dataset != 'PD':
				self.samples = range(1,self.total_samples+1)
			else:
				self.samples = range(51,self.total_samples+1)
				self.total_samples = 50

	def GetCoords(self):
		if self.application == 'extrapolation':
			self.coords = get_mgrid([self.total_samples,self.dim[0],self.dim[1],self.dim[2]],dim=4,s=1,t=0)
			self.coords = self.coords[0:self.training_samples*self.dim[0]*self.dim[1]*self.dim[2]:,]
		elif self.application == "temporal":
			if self.dataset != 'Earthquake':
				self.coords = get_mgrid([self.total_samples,self.dim[0],self.dim[1],self.dim[2]],dim=4,s=1,t=self.interval)
			else:
				coords = []
				time = np.zeros((self.dim[0]*self.dim[1]*self.dim[2],1))
				self.coords = get_mgrid([self.dim[0],self.dim[1],self.dim[2]],dim=3)
				for t in self.samples:
					if self.dataset == 'Earthquake':
						t = (t-1)/(self.total_samples-1)
					else:
						t = t/(self.total_samples-1)
					t -= 0.5
					t *= 2
					time.fill(t)
					coords += list(np.concatenate((time,self.coords),axis=1))
				self.coords = np.asarray(coords)
		elif self.application == 'spatial':
			self.Subsample()
			self.coords = get_mgrid([self.total_samples,self.dim[0],self.dim[1],self.dim[2]],dim=4,s=self.scale,t=0)
		elif self.application == 'super-spatial':
			self.coords = get_mgrid([self.total_samples,self.dim[0]*self.scale,self.dim[1]*self.scale,self.dim[2]*self.scale],dim=4,s=self.scale,t=0)

	def ReadData(self):
		self.GetCoords()
		self.data = []
		for i in self.samples:
			if self.dataset!='Tangaroa-M':
				#d = np.fromfile(self.data_path)
				# Read the binary data from the file
				with open(self.data_path+'/Pf'+'{:02d}'.format(i)+'.binLE.raw_corrected_2_subsampled', 'rb') as f:
						raw_data = f.read()
				d= np.frombuffer(raw_data, dtype=np.float32)
			else:
				d = np.fromfile(self.data_path+'{:04d}'.format(i+50)+'.dat',dtype='<f')
			d = 2*(d-np.min(d))/(np.max(d)-np.min(d))-1
			if self.application == 'spatial':
				self.data += list(d[self.coords_indices])
			else:
				self.data += list(d)
		self.data = np.asarray(self.data)

	def Subsample(self):
		self.coords_indices = []
		for z in range(0,self.dim[2],self.scale):
			for y in range(0,self.dim[1],self.scale):
				for x in range(0,self.dim[0],self.scale):
					index = (((z) * self.dim[1] + y) * self.dim[0] + x)
					self.coords_indices.append(index)
		self.coords_indices = np.asarray(self.coords_indices)


	def GetTrainingData(self):
		indices = []
		if self.application == 'spatial':
			samples = (self.dim[0]*self.dim[1]*self.dim[2])//(self.scale*self.scale*self.scale)
		elif self.application == 'super-spatial':
			samples = (self.dim[0]*self.dim[1]*self.dim[2])
		else:
			samples = self.dim[0]*self.dim[1]*self.dim[2]


		if self.application == 'extrapolation':
			for i in range(0,self.training_samples):
				index = np.random.choice(np.arange(i*samples,(i+1)*samples), self.factor*self.batch_size, replace=False)
				indices += list(index)
		elif self.application == 'temporal':
			for i in range(0,len(self.samples)):
				index = np.random.choice(np.arange(i*samples,(i+1)*samples), self.factor*self.batch_size, replace=False)
				indices += list(index)
		elif self.application == 'super-spatial':
			for i in range(0,len(self.samples)):
				index = np.random.choice(np.arange(i*samples,(i+1)*samples), self.factor*self.batch_size, replace=False)
				indices += list(index)


		if self.application in ['temporal','completion','super-spatial','extrapolation']:
			training_data_input = torch.FloatTensor(self.coords[indices])
			training_data_output = torch.FloatTensor(self.data[indices])
		elif self.application == 'spatial':
			if self.factor*self.batch_size >= samples:
				training_data_input = torch.FloatTensor(self.coords)
				training_data_output = torch.FloatTensor(self.data)
			else:
				for i in range(0,len(self.samples)):
					index = np.random.randint(low=i*samples,high=(i+1)*samples,size=self.factor*self.batch_size)
					indices += list(index)
				training_data_input = torch.FloatTensor(self.coords[indices])
				training_data_output = torch.FloatTensor(self.data[indices])
		data = torch.utils.data.TensorDataset(training_data_input,training_data_output)
		train_loader = DataLoader(dataset=data, batch_size=self.batch_size, shuffle=True)
		return train_loader

	def GetTestingData(self):
		return get_mgrid([self.total_samples,self.dim[0],self.dim[1],self.dim[2]],dim=4)

class ViewSynthesis():
	def __init__(self,args):
		self.dataset = args.dataset
		self.batch_size = args.batch_size
		self.factor = args.factor
		self.res = args.res
		self.angle_theta = args.angle_theta
		self.angle_phi = args.angle_phi
		self.path = '/data/Isabel_pressure_volume_random_train_10k'
		self.theta = [i for i in range(0,90,self.angle_theta)]
		self.phi = [i for i in range(0,360,self.angle_phi)]+[359]
		self.pathall=[]

		dataset = os.listdir(self.path)
		for data in dataset:
				path = os.path.join(self.path, data)
				self.pathall.append(path)

		if not os.path.exists(args.result_path+args.dataset):
			os.mkdir(args.result_path+args.dataset)

		if not os.path.exists(args.model_path+args.dataset):
			os.mkdir(args.model_path+args.dataset)

	def ReadData(self):
		self.count = 0
		self.coords = get_mgrid([self.res,self.res],dim=2)
		theta = np.zeros((self.res*self.res,1))
		phi = np.zeros((self.res*self.res,1))
		self.R = []
		self.G = []
		self.B = []
		coords = []
		print(self.phi,self.theta)
		for t in self.theta:
			theta_ = t/179.0
			theta_ -= 0.5
			theta_ *= 2.0
			theta.fill(theta_)
			for p in self.phi:
				phi_ = p/359.0
				phi_ -= 0.5
				phi_ *= 2.0
				phi.fill(phi_)
				coords += list(np.concatenate((self.coords,theta,phi),axis=1))
				img = img_as_float(imread(self.pathall[self.count]))
				img = img.transpose(2,0,1)
				img -= 0.5
				img *= 2.0
				self.R += list(img[0].flatten('F'))
				self.G += list(img[1].flatten('F'))
				self.B += list(img[2].flatten('F'))
				self.count += 1
				print(self.count)
		self.pixels = np.asarray([self.R,self.G,self.B])
		self.pixels = np.transpose(self.pixels,(1,0))
		self.coords = np.asarray(coords)

	def GetTrainingData(self):
		samples = self.res*self.res
		indices = []
		for i in range(0,self.count):
			index = np.random.choice(np.arange(i*samples,(i+1)*samples), self.factor*self.batch_size, replace=False)
			indices += list(index)
		training_data_input = torch.FloatTensor(self.coords[indices])
		training_data_output = torch.FloatTensor(self.pixels[indices])

		data = torch.utils.data.TensorDataset(training_data_input,training_data_output)
		train_loader = DataLoader(dataset=data, batch_size=self.batch_size, shuffle=True)
		return train_loader
